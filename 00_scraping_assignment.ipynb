{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "#from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-search-engine-choice-screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open page\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(\"https://digital.oegbverlag.at/login?r=https%3A%2F%2Fdigital.oegbverlag.at%2Fkvsystem\")\n",
    "\n",
    "#for some reason I have to manually click on the \"reject cookies\" option, can't find a way to make the webdriver do it \n",
    "#element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, '//*[@id=\"uc-center-container\"]/div[2]/div/div/div/div/button[2]')))\n",
    "#select_element = driver.find_element(By.XPATH, '//*[@id=\"uc-center-container\"]/div[2]/div/div/div/div/button[2]')\n",
    "#select_element.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#select \"für ögbverlag kunden\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m element\u001b[38;5;241m=\u001b[39m\u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/html/body/div[1]/div/div[2]/div/div[1]/div[3]/div[2]/button\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m select_element \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div[1]/div/div[2]/div/div[1]/div[3]/div[2]/button\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m select_element\u001b[38;5;241m.\u001b[39mclick()\n",
      "File \u001b[1;32mc:\\Users\\jakob\\miniconda3\\envs\\assignment_env\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#select \"für ögbverlag kunden\"\n",
    "element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div/div[1]/div[3]/div[2]/button\")))\n",
    "select_element = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[2]/div/div[1]/div[3]/div[2]/button')\n",
    "select_element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "password = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "username.send_keys(\"jakob.wall@econ.uzh.ch\")\n",
    "password.send_keys(\"D00rcityoverhere\")\n",
    "\n",
    "driver.find_element(By.ID, \"login\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select \"erweiterte Suche and search\"\n",
    "element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div/div[2]\")))\n",
    "select_element = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div/div[2]')\n",
    "select_element.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we only loop over the first year, the real code does every year up to 2021\n",
    "all_years = []\n",
    "for year in range(1995,1996):\n",
    "    driver.get(\"https://digital.oegbverlag.at/kvsystem?q=&v=\"+str(year)+\"-01-01\")\n",
    "\n",
    "    #search\n",
    "    element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[1]/div/input\")))\n",
    "    searchbar = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/div[1]/div[1]/div/input\")\n",
    "    searchbar.send_keys(Keys.RETURN)\n",
    "\n",
    "    #I need to click on \"mehr anzeigen\" until the whole list is visible. \n",
    "    #Unfortunately, the site does not work perfectly, and keeps on displaying the button \"mehr anzeigen\" as a clickable element even though we are at the end. As a solution, I just clik 40 times, which is always enough\n",
    "    for i in range (40):\n",
    "        element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div/div[2]/button\")))\n",
    "        select_element = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[2]/div[2]/div/div[2]/button')\n",
    "        select_element.click()\n",
    "        #need to wait a bit, otherwise the site doesn't respond\n",
    "        time.sleep(2)\n",
    "        \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('a')\n",
    "    # Create a list to store (text, link) tuples\n",
    "    result_list = []\n",
    "    # Loop through each <a> tag\n",
    "    for link in links:\n",
    "        # Get the href attribute (the link)\n",
    "        href = link.get('href')\n",
    "        \n",
    "        # Get the text inside the <span> (if any)\n",
    "        text = link.get_text().strip()  # .strip() removes any extra whitespace\n",
    "        \n",
    "        if href and text:\n",
    "            result_list.append((text, href))\n",
    "\n",
    "    # Filter the list to keep only elements where the link starts with \"/kvsystem\"\n",
    "    filtered_list = [(text, link) for text, link in result_list if link.startswith(\"/kvsystem/\")]\n",
    "\n",
    "    #append current year\n",
    "    all_years.append(filtered_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn list into pandas dataframe \n",
    "all_years_df = pd.DataFrame(columns = ['Text', 'Link', 'Year'])\n",
    "for i in range(len(all_years)):\n",
    "    df_current = pd.DataFrame(all_years[i], columns=['Text', 'Link'])\n",
    "    df_current['Year'] = 1995+i\n",
    "\n",
    "    all_years_df = pd.concat([all_years_df,df_current ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "#here we only save a sample\n",
    "all_years_df.to_csv('./data/kv_links_all_years_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load csv and sort\n",
    "all_years_df = pd.read_csv('./data/kv_links_all_years_sample.csv')\n",
    "all_years_df = all_years_df.sort_values('Text')\n",
    "all_years_df = all_years_df.reset_index(drop=True)\n",
    "all_years_df = all_years_df.drop(columns =['Unnamed: 0'])\n",
    "\n",
    "#ignore zusatz kvs\n",
    "all_years_df = all_years_df[~all_years_df['Text'].str.contains(\"ZKV\")]\n",
    "all_years_df = all_years_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#the real codes loops over all links, here we only show the process for 3\n",
    "#for i in range(1,len(all_years_df)):\n",
    "for i in range(1,3):\n",
    "    link = str(\"https://digital.oegbverlag.at\" + all_years_df['Link'].iloc[i])\n",
    "\n",
    "    driver.get(link)\n",
    "    time.sleep(1.5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #check if we get a server error: if yes, we skip to the next link\n",
    "    is_server_error = 1 if \"Serverfehler\" in soup.title.string else 0\n",
    "    if is_server_error:\n",
    "        print(\"Server Error\")\n",
    "\n",
    "    else:\n",
    "        #'_c._d._e._f._h._av._al'\n",
    "        #'_go._p._c._d._e._ap._gn._i._bs'\n",
    "        element=WebDriverWait(driver, 300).until(EC. element_to_be_clickable((By.CLASS_NAME, '_c._d._e._f._h._av._al')))\n",
    "        xpath = \"//div[contains(concat(' ', normalize-space(@class), ' '), ' _c ') and contains(concat(' ', normalize-space(@class), ' '), ' _d ') and contains(concat(' ', normalize-space(@class), ' '), ' _e ')]/div[contains(concat(' ', normalize-space(@class), ' '), ' _c ') and contains(concat(' ', normalize-space(@class), ' '), ' _d ') and contains(concat(' ', normalize-space(@class), ' '), ' _e ') and contains(concat(' ', normalize-space(@class), ' '), ' _f ') and contains(concat(' ', normalize-space(@class), ' '), ' _h ') and contains(concat(' ', normalize-space(@class), ' '), ' _av ') and contains(concat(' ', normalize-space(@class), ' '), ' _al ')]\"\n",
    "        subcontracts = driver.find_elements(By.XPATH, xpath)\n",
    "        #subcontracts = driver.find_elements(By.CLASS_NAME, '_c._d._e._f._h._av._al')\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        css_selector = f\".{'_c._d._e'} > .{'_c._d._e._f._h._av._al'}\"\n",
    "        subcontracts_soup = soup.select(css_selector)\n",
    "\n",
    "        # Step 3: Loop over the subparts\n",
    "        contract_df = pd.DataFrame(columns= ['Text', 'Link', 'Year', 'subcontract_html', 'subcontract_text'])\n",
    "        subcontracts_loop = subcontracts\n",
    "        subcontract_df = pd.DataFrame(columns=['contract', 'year', 'subcontract_title', 'subcontract_html', 'subcontract_text'])\n",
    "        for subcontract_nr in range(len(subcontracts)):\n",
    "            #print(subcontract_nr)\n",
    "            #load page\n",
    "            driver.get(link)\n",
    "            element=WebDriverWait(driver, 300).until(EC. element_to_be_clickable((By.CLASS_NAME, '_c._d._e._f._h._av._al')))\n",
    "            subcontracts_loop = driver.find_elements(By.XPATH, xpath)\n",
    "\n",
    "            subcontract = subcontracts_loop[subcontract_nr]\n",
    "            # Step 3a: Click on the subpart to load its content\n",
    "            subcontract.click()\n",
    "\n",
    "            # Wait for the page to load \n",
    "            element=WebDriverWait(driver, 300).until(EC. element_to_be_clickable((By.CLASS_NAME, '_c._d._e._f._h._av._al')))\n",
    "\n",
    "            #Step 3b: Scrape the displayed text on the subpart\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            #paragraphs = soup.find_all(class_=re.compile(r'\\bcp_digital_para para pr-'))\n",
    "\n",
    "            #print(paragraphs)\n",
    "\n",
    "            #select text of the actual subcontract\n",
    "            actual_subcontract = soup.find(class_=re.compile(r'\\bcp_digital_para cp_digital'))\n",
    "\n",
    "            subcontract_df.loc[subcontract_nr,'contract'] = all_years_df.loc[i,'Text']\n",
    "            subcontract_df.loc[subcontract_nr,'year'] = all_years_df.loc[i,'Year'] \n",
    "            subcontract_df.loc[subcontract_nr,'subcontract_title'] = subcontracts_soup[subcontract_nr].find('a').get_text()\n",
    "            subcontract_df.loc[subcontract_nr,'subcontract_html'] = str(actual_subcontract)\n",
    "            subcontract_df.loc[subcontract_nr, 'subcontract_text'] = actual_subcontract.get_text()\n",
    "\n",
    "        contracts_df = pd.concat([contracts_df, subcontract_df], ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to main contract (\"Rahmen\") only\n",
    "contracts_df = contracts_df[['contract', 'year', 'subcontract_title', 'subcontract_html', 'subcontract_text']]\n",
    "\n",
    "rahmen_all_df = contracts_df[contracts_df['subcontract_title'] == 'Rahmen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "#we again only save a sample\n",
    "contracts_df.to_csv('./data/kv_contracts_all_years_sample.csv', index=False)\n",
    "rahmen_all_df.to_csv('./data/kv_contracts_rahmen_all_years_sample.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cb_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
