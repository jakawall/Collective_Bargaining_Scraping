{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-search-engine-choice-screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open page\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(\"https://digital.oegbverlag.at/login?r=https%3A%2F%2Fdigital.oegbverlag.at%2Fkvsystem\")\n",
    "time.sleep(3)\n",
    "\n",
    "#for some reason I have to manually click on the \"reject cookies\" option, can't find a way to make the webdriver do it \n",
    "#select_element = driver.find_element(By.XPATH, '//*[@id=\"uc-center-container\"]/div[2]/div/div/div/div/button[2]')\n",
    "#select_element.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select \"für ögbverlag kunden\"\n",
    "element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div/div[1]/div[3]/div[2]/button\")))\n",
    "select_element = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[2]/div/div[1]/div[3]/div[2]/button')\n",
    "select_element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "password = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "username.send_keys(\"jakob.wall@econ.uzh.ch\")\n",
    "password.send_keys(\"D00rcityoverhere\")\n",
    "\n",
    "driver.find_element(By.ID, \"login\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select \"erweiterte Suche and search\"\n",
    "element=WebDriverWait(driver, 10).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div/div[2]\")))\n",
    "select_element = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div/div[2]')\n",
    "select_element.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = []\n",
    "for year in range(1995,2022):\n",
    "    driver.get(\"https://digital.oegbverlag.at/kvsystem?q=&v=\"+str(year)+\"-01-01\")\n",
    "    #print(\"01.01.\" + str(year))\n",
    "\n",
    "    #select date\n",
    "    #date = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div[2]/div/div/div[1]/div[1]/div/div/div[2]/div/input\")\n",
    "    #date.send_keys(Keys.CONTROL + \"a\")\n",
    "    #date.send_keys(Keys.DELETE)\n",
    "    #date.send_keys(\"01.01.\" + str(year))\n",
    "\n",
    "    #search\n",
    "    element=WebDriverWait(driver, 30000000).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[1]/div/input\")))\n",
    "    searchbar = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/div[1]/div[1]/div/input\")\n",
    "    searchbar.send_keys(Keys.RETURN)\n",
    "\n",
    "    #I need to click on \"mehr anzeigen\" until the whole list is visible. \n",
    "    #Unfortunately, the site does not work perfectly, and keeps on displaying the button \"mehr anzeigen\" as a clickable element even though we are at the end. As a solution, I just clik 40 times, which is always enough\n",
    "    #  Unfortunately, the site does not work perf\n",
    "    #select \"mehr anzeigen\"\n",
    "    for i in range (40):\n",
    "        element=WebDriverWait(driver, 30000000).until(EC. element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[2]/div[2]/div/div[2]/button\")))\n",
    "        select_element = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[2]/div[2]/div/div[2]/button')\n",
    "        select_element.click()\n",
    "        #need to wait a bit, otherwise the site doesn't respond\n",
    "        time.sleep(2)\n",
    "        print(i)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('a')\n",
    "    # Create a list to store (text, link) tuples\n",
    "    result_list = []\n",
    "    # Loop through each <a> tag\n",
    "    for link in links:\n",
    "        # Get the href attribute (the link)\n",
    "        href = link.get('href')\n",
    "        \n",
    "        # Get the text inside the <span> (if any)\n",
    "        text = link.get_text().strip()  # .strip() removes any extra whitespace\n",
    "        \n",
    "        if href and text:\n",
    "            result_list.append((text, href))\n",
    "\n",
    "    # Filter the list to keep only elements where the link starts with \"/kvsystem\"\n",
    "    filtered_list = [(text, link) for text, link in result_list if link.startswith(\"/kvsystem/\")]\n",
    "\n",
    "    #append current year\n",
    "    all_years.append(filtered_list)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn list into pandas dataframe \n",
    "all_years_df = pd.DataFrame(columns = ['Text', 'Link', 'Year'])\n",
    "for i in range(len(all_years)):\n",
    "    df_current = pd.DataFrame(all_years[i], columns=['Text', 'Link'])\n",
    "    df_current['Year'] = 1995+i\n",
    "\n",
    "    all_years_df = pd.concat([all_years_df,df_current ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "all_years_df.to_csv('./data/kv_links_all_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load csv and sort\n",
    "all_years_df = pd.read_csv('./data/kv_links_all_years.csv')\n",
    "all_years_df = all_years_df.sort_values('Text')\n",
    "all_years_df = all_years_df.reset_index(drop=True)\n",
    "all_years_df = all_years_df.drop(columns =['Unnamed: 0'])\n",
    "\n",
    "#ignore zusatz kvs\n",
    "all_years_df = all_years_df[~all_years_df['Text'].str.contains(\"ZKV\")]\n",
    "all_years_df = all_years_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(all_years_df)):\n",
    "    print(i)\n",
    "    link = str(\"https://digital.oegbverlag.at\" + all_years_df['Link'].iloc[i])\n",
    "\n",
    "    driver.get(link)\n",
    "    time.sleep(1.5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #check if we get a server error: if yes, we skip to the next link\n",
    "    is_server_error = 1 if \"Serverfehler\" in soup.title.string else 0\n",
    "    if is_server_error:\n",
    "        print(\"Server Error\")\n",
    "\n",
    "    else:\n",
    "        #'_c._d._e._f._h._av._al'\n",
    "        #'_go._p._c._d._e._ap._gn._i._bs'\n",
    "        element=WebDriverWait(driver, 300).until(EC. element_to_be_clickable((By.CLASS_NAME, '_c._d._e._f._h._av._al')))\n",
    "        xpath = \"//div[contains(concat(' ', normalize-space(@class), ' '), ' _c ') and contains(concat(' ', normalize-space(@class), ' '), ' _d ') and contains(concat(' ', normalize-space(@class), ' '), ' _e ')]/div[contains(concat(' ', normalize-space(@class), ' '), ' _c ') and contains(concat(' ', normalize-space(@class), ' '), ' _d ') and contains(concat(' ', normalize-space(@class), ' '), ' _e ') and contains(concat(' ', normalize-space(@class), ' '), ' _f ') and contains(concat(' ', normalize-space(@class), ' '), ' _h ') and contains(concat(' ', normalize-space(@class), ' '), ' _av ') and contains(concat(' ', normalize-space(@class), ' '), ' _al ')]\"\n",
    "        subcontracts = driver.find_elements(By.XPATH, xpath)\n",
    "        #subcontracts = driver.find_elements(By.CLASS_NAME, '_c._d._e._f._h._av._al')\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        css_selector = f\".{'_c._d._e'} > .{'_c._d._e._f._h._av._al'}\"\n",
    "        subcontracts_soup = soup.select(css_selector)\n",
    "\n",
    "        # Step 3: Loop over the subparts\n",
    "        contract_df = pd.DataFrame(columns= ['Text', 'Link', 'Year', 'subcontract_html', 'subcontract_text'])\n",
    "        subcontracts_loop = subcontracts\n",
    "        subcontract_df = pd.DataFrame(columns=['contract', 'year', 'subcontract_title', 'subcontract_html', 'subcontract_text'])\n",
    "        for subcontract_nr in range(len(subcontracts)):\n",
    "            #print(subcontract_nr)\n",
    "            #load page\n",
    "            driver.get(link)\n",
    "            element=WebDriverWait(driver, 300).until(EC. element_to_be_clickable((By.CLASS_NAME, '_c._d._e._f._h._av._al')))\n",
    "            subcontracts_loop = driver.find_elements(By.XPATH, xpath)\n",
    "\n",
    "            subcontract = subcontracts_loop[subcontract_nr]\n",
    "            # Step 3a: Click on the subpart to load its content\n",
    "            subcontract.click()\n",
    "\n",
    "            # Wait for the page to load \n",
    "            element=WebDriverWait(driver, 300).until(EC. element_to_be_clickable((By.CLASS_NAME, '_c._d._e._f._h._av._al')))\n",
    "\n",
    "            #Step 3b: Scrape the displayed text on the subpart\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            #paragraphs = soup.find_all(class_=re.compile(r'\\bcp_digital_para para pr-'))\n",
    "\n",
    "            #print(paragraphs)\n",
    "\n",
    "            #select text of the actual subcontract\n",
    "            actual_subcontract = soup.find(class_=re.compile(r'\\bcp_digital_para cp_digital'))\n",
    "\n",
    "            subcontract_df.loc[subcontract_nr,'contract'] = all_years_df.loc[i,'Text']\n",
    "            subcontract_df.loc[subcontract_nr,'year'] = all_years_df.loc[i,'Year'] \n",
    "            subcontract_df.loc[subcontract_nr,'subcontract_title'] = subcontracts_soup[subcontract_nr].find('a').get_text()\n",
    "            subcontract_df.loc[subcontract_nr,'subcontract_html'] = str(actual_subcontract)\n",
    "            subcontract_df.loc[subcontract_nr, 'subcontract_text'] = actual_subcontract.get_text()\n",
    "\n",
    "\n",
    "        contracts_df = pd.concat([contracts_df, subcontract_df], ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to main contract (\"Rahmen\") only\n",
    "contracts_df = contracts_df[['contract', 'year', 'subcontract_title', 'subcontract_html', 'subcontract_text']]\n",
    "\n",
    "rahmen_all_df = contracts_df[contracts_df['subcontract_title'] == 'Rahmen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "#contracts_df.to_csv('./data/kv_contracts_all_years.csv', index=False)\n",
    "#rahmen_all_df.to_csv('./data/kv_contracts_rahmen_all_years.csv', index=False)\n",
    "contracts_df = pd.read_csv('./data/kv_contracts_all_years.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cb_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
